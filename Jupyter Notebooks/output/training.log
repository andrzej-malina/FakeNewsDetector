2021-02-07 19:38:48,117 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,119 Model: "TextClassifier(
  (document_embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (1): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (2): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (3): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (4): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (5): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=2, bias=True)
  (loss_function): BCEWithLogitsLoss()
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-02-07 19:38:48,120 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,121 Corpus: "Corpus: 3889 train + 487 dev + 486 test sentences"
2021-02-07 19:38:48,122 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,123 Parameters:
2021-02-07 19:38:48,124  - learning_rate: "3e-05"
2021-02-07 19:38:48,125  - mini_batch_size: "16"
2021-02-07 19:38:48,158  - patience: "3"
2021-02-07 19:38:48,159  - anneal_factor: "0.5"
2021-02-07 19:38:48,160  - max_epochs: "5"
2021-02-07 19:38:48,161  - shuffle: "True"
2021-02-07 19:38:48,162  - train_with_dev: "False"
2021-02-07 19:38:48,164  - batch_growth_annealing: "False"
2021-02-07 19:38:48,165 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,165 Model training base path: "..\output"
2021-02-07 19:38:48,166 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,167 Device: cpu
2021-02-07 19:38:48,167 ----------------------------------------------------------------------------------------------------
2021-02-07 19:38:48,168 Embeddings storage mode: cpu
2021-02-07 19:38:48,172 ----------------------------------------------------------------------------------------------------
2021-02-07 19:41:44,257 epoch 1 - iter 24/244 - loss 0.63690232 - samples/sec: 2.62 - lr: 0.000030
2021-02-07 19:44:22,674 epoch 1 - iter 48/244 - loss 0.62002986 - samples/sec: 2.43 - lr: 0.000030
2021-02-07 19:46:46,571 epoch 1 - iter 72/244 - loss 0.59764152 - samples/sec: 2.68 - lr: 0.000030
2021-02-07 19:49:53,402 epoch 1 - iter 96/244 - loss 0.59970670 - samples/sec: 2.06 - lr: 0.000030
2021-02-07 19:52:57,590 epoch 1 - iter 120/244 - loss 0.60518967 - samples/sec: 2.09 - lr: 0.000030
2021-02-07 19:56:01,076 epoch 1 - iter 144/244 - loss 0.60466835 - samples/sec: 2.09 - lr: 0.000030
2021-02-07 19:58:59,951 epoch 1 - iter 168/244 - loss 0.60081583 - samples/sec: 2.15 - lr: 0.000030
2021-02-07 20:01:55,509 epoch 1 - iter 192/244 - loss 0.59753836 - samples/sec: 2.19 - lr: 0.000030
2021-02-07 20:04:39,062 epoch 1 - iter 216/244 - loss 0.60074358 - samples/sec: 2.35 - lr: 0.000030
2021-02-07 20:07:29,624 epoch 1 - iter 240/244 - loss 0.59816511 - samples/sec: 2.25 - lr: 0.000030
2021-02-07 20:07:57,882 ----------------------------------------------------------------------------------------------------
2021-02-07 20:07:57,883 EPOCH 1 done: loss 0.5993 - lr 0.0000300
2021-02-07 20:13:44,610 DEV : loss 0.5883580446243286 - score 0.7077
2021-02-07 20:13:45,384 BAD EPOCHS (no improvement): 0
2021-02-07 20:13:45,970 ----------------------------------------------------------------------------------------------------
2021-02-07 20:16:55,275 epoch 2 - iter 24/244 - loss 0.53280909 - samples/sec: 2.36 - lr: 0.000030
2021-02-07 20:19:44,604 epoch 2 - iter 48/244 - loss 0.53016402 - samples/sec: 2.27 - lr: 0.000030
2021-02-07 20:22:41,860 epoch 2 - iter 72/244 - loss 0.48453398 - samples/sec: 2.17 - lr: 0.000030
2021-02-07 20:25:33,778 epoch 2 - iter 96/244 - loss 0.47181618 - samples/sec: 2.23 - lr: 0.000030
2021-02-07 20:28:29,040 epoch 2 - iter 120/244 - loss 0.46704392 - samples/sec: 2.20 - lr: 0.000030
2021-02-07 20:31:18,424 epoch 2 - iter 144/244 - loss 0.46083625 - samples/sec: 2.27 - lr: 0.000030
2021-02-07 20:34:08,914 epoch 2 - iter 168/244 - loss 0.46346629 - samples/sec: 2.25 - lr: 0.000030
2021-02-07 20:37:02,517 epoch 2 - iter 192/244 - loss 0.45273616 - samples/sec: 2.21 - lr: 0.000030
2021-02-07 20:40:04,415 epoch 2 - iter 216/244 - loss 0.46122232 - samples/sec: 2.12 - lr: 0.000030
2021-02-07 20:42:57,154 epoch 2 - iter 240/244 - loss 0.45955358 - samples/sec: 2.22 - lr: 0.000030
2021-02-07 20:43:30,330 ----------------------------------------------------------------------------------------------------
2021-02-07 20:43:30,333 EPOCH 2 done: loss 0.4589 - lr 0.0000300
2021-02-07 20:50:01,025 DEV : loss 0.6523706316947937 - score 0.7058
2021-02-07 20:50:01,829 BAD EPOCHS (no improvement): 1
2021-02-07 20:50:01,830 ----------------------------------------------------------------------------------------------------
2021-02-07 20:53:28,425 epoch 3 - iter 24/244 - loss 0.29078802 - samples/sec: 2.16 - lr: 0.000030
2021-02-07 20:56:21,891 epoch 3 - iter 48/244 - loss 0.28294939 - samples/sec: 2.21 - lr: 0.000030
2021-02-07 20:59:16,520 epoch 3 - iter 72/244 - loss 0.33139059 - samples/sec: 2.20 - lr: 0.000030
2021-02-07 21:01:48,308 epoch 3 - iter 96/244 - loss 0.31210108 - samples/sec: 2.54 - lr: 0.000030
2021-02-07 21:04:47,459 epoch 3 - iter 120/244 - loss 0.31651950 - samples/sec: 2.14 - lr: 0.000030
2021-02-07 21:08:26,915 epoch 3 - iter 144/244 - loss 0.28871864 - samples/sec: 1.75 - lr: 0.000030
2021-02-07 21:11:37,219 epoch 3 - iter 168/244 - loss 0.28852520 - samples/sec: 2.02 - lr: 0.000030
2021-02-07 21:14:36,324 epoch 3 - iter 192/244 - loss 0.29054259 - samples/sec: 2.15 - lr: 0.000030
2021-02-07 21:17:27,128 epoch 3 - iter 216/244 - loss 0.28101316 - samples/sec: 2.25 - lr: 0.000030
2021-02-07 21:20:21,905 epoch 3 - iter 240/244 - loss 0.28471533 - samples/sec: 2.20 - lr: 0.000030
2021-02-07 21:20:48,852 ----------------------------------------------------------------------------------------------------
2021-02-07 21:20:48,853 EPOCH 3 done: loss 0.2847 - lr 0.0000300
2021-02-07 21:27:27,198 DEV : loss 0.6490669250488281 - score 0.7109
2021-02-07 21:27:28,061 BAD EPOCHS (no improvement): 0
2021-02-07 21:27:28,679 ----------------------------------------------------------------------------------------------------
2021-02-07 21:30:48,471 epoch 4 - iter 24/244 - loss 0.19423113 - samples/sec: 2.24 - lr: 0.000030
2021-02-07 21:33:42,638 epoch 4 - iter 48/244 - loss 0.17381451 - samples/sec: 2.21 - lr: 0.000030
2021-02-07 21:36:24,507 epoch 4 - iter 72/244 - loss 0.14477541 - samples/sec: 2.37 - lr: 0.000030
2021-02-07 21:39:03,204 epoch 4 - iter 96/244 - loss 0.15663733 - samples/sec: 2.42 - lr: 0.000030
2021-02-07 21:41:59,177 epoch 4 - iter 120/244 - loss 0.15017317 - samples/sec: 2.19 - lr: 0.000030
2021-02-07 21:44:34,776 epoch 4 - iter 144/244 - loss 0.15050174 - samples/sec: 2.47 - lr: 0.000030
2021-02-07 21:47:08,630 epoch 4 - iter 168/244 - loss 0.15488512 - samples/sec: 2.50 - lr: 0.000030
2021-02-07 21:49:39,775 epoch 4 - iter 192/244 - loss 0.15296273 - samples/sec: 2.54 - lr: 0.000030
2021-02-07 21:52:14,895 epoch 4 - iter 216/244 - loss 0.15720182 - samples/sec: 2.48 - lr: 0.000030
2021-02-07 21:54:50,563 epoch 4 - iter 240/244 - loss 0.14654700 - samples/sec: 2.47 - lr: 0.000030
2021-02-07 21:55:19,623 ----------------------------------------------------------------------------------------------------
2021-02-07 21:55:19,623 EPOCH 4 done: loss 0.1448 - lr 0.0000300
2021-02-07 22:01:13,163 DEV : loss 1.2346436977386475 - score 0.6997
2021-02-07 22:01:14,177 BAD EPOCHS (no improvement): 1
2021-02-07 22:01:14,179 ----------------------------------------------------------------------------------------------------
2021-02-07 22:04:33,687 epoch 5 - iter 24/244 - loss 0.02284955 - samples/sec: 2.23 - lr: 0.000030
2021-02-07 22:07:09,902 epoch 5 - iter 48/244 - loss 0.05093864 - samples/sec: 2.46 - lr: 0.000030
2021-02-07 22:09:47,783 epoch 5 - iter 72/244 - loss 0.06518617 - samples/sec: 2.43 - lr: 0.000030
2021-02-07 22:12:32,312 epoch 5 - iter 96/244 - loss 0.05408240 - samples/sec: 2.34 - lr: 0.000030
2021-02-07 22:15:20,538 epoch 5 - iter 120/244 - loss 0.06798680 - samples/sec: 2.28 - lr: 0.000030
2021-02-07 22:17:57,446 epoch 5 - iter 144/244 - loss 0.06273058 - samples/sec: 2.45 - lr: 0.000030
2021-02-07 22:20:37,517 epoch 5 - iter 168/244 - loss 0.06511759 - samples/sec: 2.41 - lr: 0.000030
2021-02-07 22:23:13,843 epoch 5 - iter 192/244 - loss 0.05882452 - samples/sec: 2.46 - lr: 0.000030
2021-02-07 22:26:14,902 epoch 5 - iter 216/244 - loss 0.05448304 - samples/sec: 2.12 - lr: 0.000030
2021-02-07 22:29:06,538 epoch 5 - iter 240/244 - loss 0.05215007 - samples/sec: 2.24 - lr: 0.000030
2021-02-07 22:29:35,886 ----------------------------------------------------------------------------------------------------
2021-02-07 22:29:35,888 EPOCH 5 done: loss 0.0544 - lr 0.0000300
2021-02-07 22:35:33,552 DEV : loss 1.6919775009155273 - score 0.6988
2021-02-07 22:35:34,378 BAD EPOCHS (no improvement): 2
2021-02-07 22:35:34,956 ----------------------------------------------------------------------------------------------------
2021-02-07 22:35:34,958 Testing using best model ...
2021-02-07 22:35:34,958 loading file ..\output\best-model.pt
2021-02-07 22:36:36,448 0.7103	0.7037	0.7065	0.7058
2021-02-07 22:36:36,448 
Results:
- F-score (micro) 0.7209
- F-score (macro) 0.7065
- Accuracy 0.7058

By class:
              precision    recall  f1-score   support

           0     0.7558    0.7869    0.7710       291
           1     0.6648    0.6205    0.6419       195

   micro avg     0.7216    0.7202    0.7209       486
   macro avg     0.7103    0.7037    0.7065       486
weighted avg     0.7193    0.7202    0.7192       486
 samples avg     0.7130    0.7202    0.7154       486

2021-02-07 22:36:36,448 ----------------------------------------------------------------------------------------------------
